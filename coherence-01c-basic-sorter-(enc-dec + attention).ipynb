{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: sorting digits (e.g. [3,1,2] -> [1,2,3])\n",
    "\n",
    "* Encoder-Decoder + Attention (Bahdanau et al. 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add custom import path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jacobsuwang/Documents/UTA2018/NEURAL-NETS/ATTENTION/CODE/01-import-folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "VOCAB = set(['PAD','EOS','1','2','3','4','5','6','7','8','9','0'])\n",
    "NUMBERS = ['1','2','3','4','5','6','7','8','9','0']\n",
    "MAX_LEN = len(NUMBERS) + 2\n",
    "# WORD2IDX = {'PAD':0,'EOS':1,'1':2,'2':3,'3':4,'4':5,'5':6,'6':7,'7':8,'8':9,'9':10,'0':11}\n",
    "WORD2IDX = {'PAD':0,'EOS':1,'1':2,'2':3,'3':4,'4':5,'5':6,'6':7,'7':8,'8':9,'9':10,'0':11}\n",
    "IDX2WORD = {idx:word for word,idx in WORD2IDX.iteritems()}\n",
    "# BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "def random_datum(n):\n",
    "    input_seq = list(np.random.choice(NUMBERS, n, replace=False)) \n",
    "        # e.g. ['5', '6', '3', '9', '1'].\n",
    "    sorted_seq = sorted(input_seq)\n",
    "    output_seq = [input_seq.index(word)+2 for word in sorted_seq]\n",
    "        # index in ascending.\n",
    "        # e.g. [4, 2, 0, 1, 3], for the input above.\n",
    "    input_seq = input_seq\n",
    "    return input_seq, output_seq\n",
    "\n",
    "def encode_seq(seq):\n",
    "    return [WORD2IDX[word] for word in seq]\n",
    "\n",
    "def decode_seq(seq):\n",
    "    return [IDX2WORD[idx] for idx in seq]\n",
    "\n",
    "def decode_order(seq):\n",
    "    return [aug_idx-2 for aug_idx in seq]\n",
    "\n",
    "def decode_by_index(seq, idx_seq, end_idx, correction=-2): # -2: PAD and EOS\n",
    "    decoded = []\n",
    "    for idx in range(end_idx):\n",
    "        decoded.append(seq[idx_seq[idx]+correction])\n",
    "    return decoded\n",
    "\n",
    "def random_batch(batch_size, input_length_from=2, input_length_to=MAX_LEN-2):\n",
    "    if input_length_from >= input_length_to:\n",
    "        raise ValueError('length_from >= length_to')\n",
    "    input_lengths = np.random.randint(input_length_from, input_length_to, size=batch_size)\n",
    "    input_batch, output_batch = [], []\n",
    "    for length in input_lengths:\n",
    "        input_seq, output_seq = random_datum(length)\n",
    "        input_batch.append(encode_seq(input_seq))\n",
    "        output_batch.append(output_seq)\n",
    "    return input_batch, output_batch    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.contrib.layers import safe_embedding_lookup_sparse as embedding_lookup_unique\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple, GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Graph\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Set configuration\n",
    "\n",
    "# digit_from = 2\n",
    "# digit_to = 6+1\n",
    "# word_from = 7\n",
    "# word_to = 11+1\n",
    "# seqlen_from = 3\n",
    "# seqlen_to = 8\n",
    "\n",
    "batch_size = 10 \n",
    "vocab_size = len(VOCAB)\n",
    "input_embedding_size = 10\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2 # because encoder is going to be bidirectional.\n",
    "\n",
    "attention = True     # togglable\n",
    "bidirectional = True # currently hardcoded\n",
    "\n",
    "encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "\n",
    "# _init_placeholder()\n",
    "\n",
    "encoder_inputs = tf.placeholder(shape=(None,None), dtype=tf.int32, name='encoder_inputs') # [max_time, batch_size]\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length') \n",
    "decoder_targets = tf.placeholder(shape=(None,None), dtype=tf.int32, name='decoder_targets')\n",
    "decoder_targets_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='decoder_inputs_length')\n",
    "\n",
    "# _init_decoder_train_connectors()\n",
    "#  - adding EOS to placehoder (and get seqlen right).\n",
    "\n",
    "with tf.name_scope('DecoderTrainFeeds'):\n",
    "    sequence_size, batch_size_ = tf.unstack(tf.shape(decoder_targets)) # [max_time, batch_size]\n",
    "    EOS_SLICE = tf.ones([1, batch_size_], dtype=tf.int32) * WORD2IDX['EOS']\n",
    "    PAD_SLICE = tf.ones([1, batch_size_], dtype=tf.int32) * WORD2IDX['PAD']\n",
    "    decoder_train_inputs = tf.concat([EOS_SLICE, decoder_targets], axis=0) \n",
    "        # add EOS to the beginning.\n",
    "        # node that this only changes the structure of the graph, not real input.\n",
    "    decoder_train_length = decoder_targets_length + 1 # and adjust length accordingly. \n",
    "#     decoder_train_length = decoder_targets_length # and adjust length accordingly.     \n",
    "    decoder_train_targets = tf.concat([decoder_targets, PAD_SLICE], axis=0) # add PAD to the end.\n",
    "    decoder_train_targets_seq_len, _ = tf.unstack(tf.shape(decoder_train_targets)) # seq_len = max_time\n",
    "    # the mask picks out EOS to 1 (the rest 0)\n",
    "    # so later when you add it to decoder_train_targets,\n",
    "    # the end-of-seq PAD placeholder will be replaced\n",
    "    # by 1, which is EOS.\n",
    "    decoder_train_targets_eos_mask = tf.one_hot(decoder_train_length - 1, # indices of EOS tokens.\n",
    "                                                decoder_train_targets_seq_len, # depth: dim of one-hot vecs.\n",
    "                                                on_value=WORD2IDX['EOS'], off_value=WORD2IDX['PAD'], # 1 for on, 0 for off.\n",
    "                                                dtype=tf.int32)\n",
    "    decoder_train_targets_eos_mask = tf.transpose(decoder_train_targets_eos_mask, [1, 0]) \n",
    "        # one_hot naturally produces [batch_size, max_time]\n",
    "        # we translate it to time-major, i.e. [max_time, batch_size]?\n",
    "    decoder_train_targets = tf.add(decoder_train_targets,\n",
    "                                   decoder_train_targets_eos_mask) # add EOS (in index) to end of target sequence\n",
    "    loss_weights = tf.ones([\n",
    "        batch_size,\n",
    "        tf.reduce_max(decoder_train_length) # max_time\n",
    "    ], dtype=tf.float32, name='loss_weights') # [batch_size, max_time]\n",
    "        # on init, all weights are equally important.\n",
    "\n",
    "# _init_embeddings()\n",
    "#  - looking up embedding matrix.\n",
    "\n",
    "with tf.variable_scope('embedding') as scope:\n",
    "    sqrt3 = math.sqrt(3) # unif(-sqrt(3),sqrt(3)) has var = 1.\n",
    "    initializer = tf.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "    embedding_matrix = tf.get_variable(\n",
    "        name='embedding_matrix',\n",
    "        shape=[vocab_size, input_embedding_size],\n",
    "        initializer=initializer,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    encoder_inputs_embedded = tf.nn.embedding_lookup(embedding_matrix, encoder_inputs)\n",
    "    decoder_train_inputs_embedded = tf.nn.embedding_lookup(embedding_matrix, decoder_train_inputs)\n",
    "        # decoder_train_inputs: decoder_targets prepended with EOS.\n",
    "\n",
    "# _init_bidirectional_encoder()\n",
    "#  - make encoder_state: [batch_size, hidden_size] \n",
    "#  - make encoder_outputs: [max_time, batch_size, emb_size]\n",
    "#  h_t = f(x_t, h_t-1)    # the LSTM cell.\n",
    "#  c = q({h_1, ..., h_T}) # the context vector.\n",
    "\n",
    "with tf.variable_scope('BidirectionalEncoder') as scope:\n",
    "    encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "    ((encoder_fw_outputs,encoder_bw_outputs), # both have [max_time, batch_size, emb_size]\n",
    "     (encoder_fw_state,encoder_bw_state)) = ( # (final) state tuples: (c=[batch_size,emb_size],h=same)\n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                            cell_bw=encoder_cell,\n",
    "                                            inputs=encoder_inputs_embedded,\n",
    "                                            sequence_length=encoder_inputs_length,\n",
    "                                            dtype=tf.float32, time_major=True)\n",
    "        )  \n",
    "    encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2) # concat on emb dim.\n",
    "    if isinstance(encoder_fw_state, LSTMStateTuple):\n",
    "        encoder_state_c = tf.concat((encoder_fw_state.c, encoder_bw_state.c), 1, name='bidirectional_concat_c')\n",
    "        encoder_state_h = tf.concat((encoder_fw_state.h, encoder_bw_state.h), 1, name='bidirectional_concat_h')\n",
    "        encoder_state = LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "    elif isinstance(encoder_fw_state, tf.Tensor):\n",
    "        encoder_state = tf.concat((encoder_fw_state, encoder_bw_state), 1, name='bidirectional_concat')\n",
    "    \n",
    "# _init_decoder()\n",
    "\n",
    "with tf.variable_scope('Decoder') as scope:\n",
    "    def output_fn(outputs):\n",
    "        return tf.contrib.layers.linear(outputs, vocab_size, scope=scope)\n",
    "    if not attention:\n",
    "        # no-attention: decoding conditions only on a lump-sum encoded final context vector c.\n",
    "        # p(y) = \\prod_t p(y_t | {y_1, ..., y_t-1}, c) # decoder's soft prediction (Bahdanau15:(2)).\n",
    "        #\n",
    "        decoder_fn_train = seq2seq.simple_decoder_fn_train(encoder_state=encoder_state)\n",
    "            # simple_decoder_fn_train: made for seq2seq.dynamic_rnn_decoder later.\n",
    "        decoder_fn_inference = seq2seq.simple_decoder_fn_inference(\n",
    "            output_fn=output_fn,\n",
    "            encoder_state=encoder_state,\n",
    "            embeddings=embedding_matrix,\n",
    "            start_of_sequence_id=WORD2IDX['EOS'],\n",
    "            end_of_sequence_id=WORD2IDX['EOS'],\n",
    "            maximum_length=tf.reduce_max(encoder_inputs_length) + 3,\n",
    "            num_decoder_symbols=vocab_size\n",
    "        )\n",
    "    else:\n",
    "        # attention: decoding conditions on a distinct context vector i at time-step i.\n",
    "        # p(y_i) = p(y_i | {y_1, ..., y_i-1}, c_i) # (Bahdanau15:(4))\n",
    "        # for this we need to feed *all* the hidden states of the encoder to the attention layer.\n",
    "        # c_i = \\sum_j a_ij * h_j, j: index over all hidden states (Bahdanau15:(5)).\n",
    "        #   a_ij = softmax(e_ij) (Bahdanau15:(6)),\n",
    "        #     interpretation: the probability that state i (dec) is aligned with state j (enc).\n",
    "        #   e_ij = a(s_i-1, h_j), where s_i-1 is the *decoder's hidden state*.\n",
    "        #     the function a(dec-hid, enc-hid) here is configured as an FFNN in Bahdanau15. \n",
    "        # \n",
    "        attention_states = encoder_state_h\n",
    "            # ematvey's original below is incorrect as per Bahdanau15,\n",
    "            # the attention layer takes the encoder *hidden states*, not *outputs*.\n",
    "            # attention_states = tf.transpose(encoder_outputs, [1, 0, 2]) # -> [batch_size, max_time, num_units]\n",
    "        (attention_keys,     # `to be compared with target states' (Q: target state? more like supervision for the attention network.)\n",
    "         attention_values,   # `to be used to construct context vectors' (i.e. c_i in equation (4,5))\n",
    "         attention_score_fn, # `to compute similarity between key and target states' (i.e. a(dec-hid,enc-hid))\n",
    "         attention_construct_fn) = seq2seq.prepare_attention( # construct_fn: build attention states (i.e. a_ij).\n",
    "            attention_states=attention_states,\n",
    "            attention_option='bahdanau',\n",
    "            num_units=decoder_hidden_units # num_units = hidden_size \n",
    "                # Q: could this be a different number?.          dec-hid  enc-hid\n",
    "                # no. because we are feeding s_i here for e_ij = a(s_i-1, h_j).\n",
    "        )\n",
    "        decoder_fn_train = seq2seq.attention_decoder_fn_train( # simple_decoder_fn + attention.\n",
    "            encoder_state=encoder_state,\n",
    "            attention_keys=attention_keys,\n",
    "            attention_values=attention_values,\n",
    "            attention_score_fn=attention_score_fn,\n",
    "            attention_construct_fn=attention_construct_fn,\n",
    "            name='attention_decoder'\n",
    "        )\n",
    "        decoder_fn_inference = seq2seq.attention_decoder_fn_inference(\n",
    "            output_fn=output_fn,\n",
    "            encoder_state=encoder_state,\n",
    "            attention_keys=attention_keys,\n",
    "            attention_values=attention_values,\n",
    "            attention_score_fn=attention_score_fn,\n",
    "            attention_construct_fn=attention_construct_fn,\n",
    "            embeddings=embedding_matrix,\n",
    "            start_of_sequence_id=WORD2IDX['EOS'],\n",
    "            end_of_sequence_id=WORD2IDX['EOS'],\n",
    "            maximum_length=tf.reduce_max(encoder_inputs_length) + 3,\n",
    "            num_decoder_symbols=vocab_size\n",
    "        )\n",
    "    (decoder_outputs_train,\n",
    "     decoder_state_train,\n",
    "     decoder_context_state_train) = (\n",
    "        seq2seq.dynamic_rnn_decoder(\n",
    "            cell=decoder_cell,\n",
    "            decoder_fn=decoder_fn_train,\n",
    "            inputs=decoder_train_inputs_embedded,\n",
    "            sequence_length=decoder_train_length,\n",
    "            time_major=True,\n",
    "            scope=scope\n",
    "        )\n",
    "    )\n",
    "    decoder_logits_train = output_fn(decoder_outputs_train)\n",
    "    decoder_prediction_train = tf.argmax(decoder_logits_train, axis=-1, name='decoder_prediction_train')\n",
    "    scope.reuse_variables()\n",
    "    (decoder_logits_inference,\n",
    "     decoder_state_inference,\n",
    "     decoder_context_state_inference) = (\n",
    "        seq2seq.dynamic_rnn_decoder(\n",
    "            cell=decoder_cell,\n",
    "            decoder_fn=decoder_fn_inference,\n",
    "            time_major=True,\n",
    "            scope=scope\n",
    "        )\n",
    "    )\n",
    "    decoder_prediction_inference = tf.argmax(decoder_logits_inference, axis=-1, name='decoder_prediction_inference')\n",
    "    \n",
    "# [SU] report accuracy here\n",
    "correct_raw = tf.cast(tf.equal(tf.cast(decoder_prediction_train, tf.int32), decoder_train_targets), tf.int32)\n",
    "mask = tf.cast(tf.not_equal(decoder_train_targets, WORD2IDX['PAD']), tf.int32) # EOSs are not 0ed out, there are BATCH_SIZE of them.\n",
    "total_seqlen = tf.cast(tf.reduce_sum(encoder_inputs_length), tf.float32)\n",
    "correct = tf.multiply(correct_raw, mask)\n",
    "accuracy = tf.cast(tf.reduce_sum(correct)-batch_size, tf.float32) / total_seqlen # -BATCH_SIZE correction\n",
    "    \n",
    "# _init_optimizer()\n",
    "\n",
    "logits = tf.transpose(decoder_logits_train, [1, 0, 2]) # to [batch_size, max_time, num_units]\n",
    "targets = tf.transpose(decoder_train_targets, [1, 0])  # to [batch_size, max_time]\n",
    "loss = seq2seq.sequence_loss(logits=logits, targets=targets, weights=loss_weights)\n",
    "    # `Weighted cross-entropy loss for a sequence of logits (per example)'\n",
    "    # logits: [batch_size, max_time, num_units]\n",
    "    # targets: [batch_size, max_time]\n",
    "    # weights: [batch_size, max_time]\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# run training\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Shape_1:0' shape=(2,) dtype=int32>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(decoder_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'unstack:1' shape=() dtype=int32>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.unstack(tf.shape(decoder_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_prediction_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.48968148232 | accuracy: -0.0895522385836\n",
      "  sample 1:\n",
      "    enc input     > ['4', '8', '2', '9', '3', '5', '1', '6', '7']\n",
      "    dec train predicted > ['4', '1', '1', '1', '7', '7', '6', '6', '6']\n",
      "  sample 2:\n",
      "    enc input     > ['3', '8', '2', '4', '7', '9', '0', '5', 'PAD']\n",
      "    dec train predicted > ['7', '0', '0', 'PAD', 'PAD', 'PAD', 'PAD', '5']\n",
      "  sample 3:\n",
      "    enc input     > ['1', '0', '5', '9', '6', '2', '7', '8', '4']\n",
      "    dec train predicted > ['0', '0', '5', '6', '8', '6', '4', '4', '4']\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.723235249519 | accuracy: 0.804878056049\n",
      "  sample 1:\n",
      "    enc input     > ['5', '4', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    dec train predicted > ['4', '5']\n",
      "  sample 2:\n",
      "    enc input     > ['5', '8', '0', '9', '4', 'PAD', 'PAD']\n",
      "    dec train predicted > ['0', '4', '5', '9', '9']\n",
      "  sample 3:\n",
      "    enc input     > ['0', '3', '6', '5', 'PAD', 'PAD', 'PAD']\n",
      "    dec train predicted > ['0', '3', '6', '6']\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.495182573795 | accuracy: 0.75\n",
      "  sample 1:\n",
      "    enc input     > ['0', '2', '7', '9', '5', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    dec train predicted > ['0', '2', '9', '9', '9']\n",
      "  sample 2:\n",
      "    enc input     > ['6', '1', '7', '3', '5', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    dec train predicted > ['1', '3', '5', '6', '7']\n",
      "  sample 3:\n",
      "    enc input     > ['7', '8', '1', '4', '6', '2', 'PAD', 'PAD', 'PAD']\n",
      "    dec train predicted > ['1', '4', '4', '6', '7', '8']\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.299835830927 | accuracy: 0.86274510622\n",
      "  sample 1:\n",
      "    enc input     > ['5', '1', '3', '2', '0', '6', '8', 'PAD', 'PAD']\n",
      "    dec train predicted > ['2', '1', '2', '3', '5', '6', '8']\n",
      "  sample 2:\n",
      "    enc input     > ['7', '5', '2', '3', '6', '1', '8', 'PAD', 'PAD']\n",
      "    dec train predicted > ['1', '2', '3', '6', '6', '7', '8']\n",
      "  sample 3:\n",
      "    enc input     > ['4', '9', '8', '3', '2', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    dec train predicted > ['3', '3', '4', '8', '9']\n",
      "\n",
      "batch 4000\n",
      "  minibatch loss: 0.267478317022 | accuracy: 0.844827592373\n",
      "  sample 1:\n",
      "    enc input     > ['6', '0', '7', '4', '8', '5', '3', '1', '9']\n",
      "    dec train predicted > ['0', '1', '5', '8', '5', '6', '8', '8', '9']\n",
      "  sample 2:\n",
      "    enc input     > ['1', '9', '8', '4', '5', '3', '7', 'PAD', 'PAD']\n",
      "    dec train predicted > ['1', '5', '4', '5', '7', '8', '9']\n",
      "  sample 3:\n",
      "    enc input     > ['9', '0', '2', '5', '3', '7', 'PAD', 'PAD', 'PAD']\n",
      "    dec train predicted > ['0', '2', '3', '5', '7', '9']\n",
      "\n",
      "batch 5000\n",
      "  minibatch loss: 0.257219344378 | accuracy: 0.847457647324\n",
      "  sample 1:\n",
      "    enc input     > ['0', '7', '9', '6', '5', '8', '3', '4', '2']\n",
      "    dec train predicted > ['0', '4', '8', '5', '6', '6', '7', '8', '9']\n",
      "  sample 2:\n",
      "    enc input     > ['4', '7', '2', '5', '8', '1', '3', 'PAD', 'PAD']\n",
      "    dec train predicted > ['1', '2', '3', '4', '5', '8', '8']\n",
      "  sample 3:\n",
      "    enc input     > ['4', '6', '7', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    dec train predicted > ['4', '6', '7']\n",
      "\n",
      "Evaluation results (on 100 batches):\n",
      "  average loss: 0.183169752359 | average accuracy 0.879507660866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_train_inputs(input_seq, target_seq):\n",
    "    # batch_enc, batch_dec = random_batch(word_from,word_to,digit_from,digit_to,seqlen_from,seqlen_to,batch_size)\n",
    "        # this is called in ematvey's code as:\n",
    "        # batch_data = next(batches)\n",
    "        # fd = model.make_train_inputs(batch_data, batch_data)\n",
    "    inputs_, inputs_length_ = utils.batch(input_seq) \n",
    "        # equiv encoder_inputs_, encoder_inputs_lengths_ = utils.batch(batch_enc)\n",
    "    targets_, targets_length_ = utils.batch(target_seq)\n",
    "        # equiv decoder_targets_, _ = utils.batch([seq + [word2idx['EOS']] + [word2idx['PAD']]*2 for seq in batch_dec])\n",
    "        # the EOS addition is done in a function above, so no need here.\n",
    "    return {\n",
    "        encoder_inputs: inputs_,\n",
    "        encoder_inputs_length: inputs_length_,\n",
    "        decoder_targets: targets_,\n",
    "        decoder_targets_length: targets_length_\n",
    "    }\n",
    "\n",
    "loss_track = []\n",
    "max_batches = 5000\n",
    "batches_in_epoch=1000\n",
    "num_test_batches = 100\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches+1):\n",
    "#         batch_enc, batch_dec = random_batch(word_from,word_to,digit_from,digit_to,seqlen_from,seqlen_to,batch_size)\n",
    "        batch_enc, batch_dec = random_batch(batch_size)\n",
    "        fd = make_train_inputs(batch_enc, batch_dec) \n",
    "            # ematvey: ..(batch_data, batch_data)\n",
    "            # because he does copy task, and i do translation.\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "#             print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            print('  minibatch loss: {} | accuracy: {}'.format(*sess.run([loss, accuracy], fd)))\n",
    "            for i, (e_in, dt_pred, length) in enumerate(zip(\n",
    "                    fd[encoder_inputs].T,\n",
    "                    sess.run(decoder_prediction_train, fd).T,\n",
    "                    sess.run(encoder_inputs_length, fd).T\n",
    "                )):\n",
    "                decoded_e_in = decode_seq(e_in)\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    enc input     > {}'.format(decoded_e_in))\n",
    "                print('    dec train predicted > {}'.format(decode_by_index(decoded_e_in, dt_pred, end_idx=length)))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print\n",
    "    \n",
    "#     # AFTER TRAING: want to match shape\n",
    "#     # dec_pred_ has an EOS at the end of seq\n",
    "#     # at the early stage, the network doesn't predict\n",
    "#     # this EOS correctly, so dec_pred_'s like, a whole bunch of random numbers.\n",
    "#     # after training, it gets good, and almost always predict EOS correctly.\n",
    "#     batch_enc, batch_dec = random_batch(batch_size)\n",
    "#     fd = make_train_inputs(batch_enc, batch_dec) \n",
    "#     enc_inp_ = sess.run(encoder_inputs, fd)\n",
    "#     dec_pred_, dec_tar_, dec_tr_tar_ = sess.run([decoder_prediction_train, decoder_targets, decoder_train_targets], fd)\n",
    "#     print 'encoder input:'\n",
    "#     print enc_inp_\n",
    "#     print enc_inp_.shape\n",
    "#     print 'decoder pred:'\n",
    "#     print dec_pred_\n",
    "#     print dec_pred_.shape\n",
    "#     print 'decoder target:'\n",
    "#     print dec_tar_\n",
    "#     print dec_tar_.shape\n",
    "#     print 'decoder train target:'\n",
    "#     print dec_tr_tar_\n",
    "#     print dec_tr_tar_.shape\n",
    "#     assert 1==0\n",
    "\n",
    "    # EVALUATE ON A BIG TEST SET\n",
    "    loss_track, accuracy_track = [], []\n",
    "    for _ in range(num_test_batches):\n",
    "        batch_enc, batch_dec = random_batch(batch_size)\n",
    "        fd = make_train_inputs(batch_enc, batch_dec) \n",
    "        l, a = sess.run([loss, accuracy], fd)\n",
    "        loss_track.append(l)\n",
    "        accuracy_track.append(a)\n",
    "    print('Evaluation results (on {} batches):'.format(num_test_batches))\n",
    "    print('  average loss: {} | average accuracy {}'.format(np.mean(loss_track), np.mean(accuracy_track)))\n",
    "    print    \n",
    "\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
