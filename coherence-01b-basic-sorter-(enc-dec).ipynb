{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: sorting digits (e.g. [3,1,2] -> [1,2,3])\n",
    "\n",
    "* Encoder-Decoder (Sutskever et al. 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add custom import path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jacobsuwang/Documents/UTA2018/NEURAL-NETS/ATTENTION/CODE/01-import-folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "VOCAB = set(['PAD','EOS','1','2','3','4','5','6','7','8','9','0'])\n",
    "NUMBERS = ['1','2','3','4','5','6','7','8','9','0']\n",
    "MAX_LEN = len(NUMBERS) + 2\n",
    "# WORD2IDX = {'PAD':0,'EOS':1,'1':2,'2':3,'3':4,'4':5,'5':6,'6':7,'7':8,'8':9,'9':10,'0':11}\n",
    "WORD2IDX = {'PAD':0,'EOS':1,'1':2,'2':3,'3':4,'4':5,'5':6,'6':7,'7':8,'8':9,'9':10,'0':11}\n",
    "IDX2WORD = {idx:word for word,idx in WORD2IDX.iteritems()}\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "def random_datum(n):\n",
    "    input_seq = list(np.random.choice(NUMBERS, n, replace=False)) \n",
    "        # e.g. ['5', '6', '3', '9', '1'].\n",
    "    sorted_seq = sorted(input_seq)\n",
    "    output_seq = [input_seq.index(word)+2 for word in sorted_seq]\n",
    "        # index in ascending.\n",
    "        # e.g. [4, 2, 0, 1, 3], for the input above.\n",
    "    input_seq = input_seq\n",
    "    return input_seq, output_seq\n",
    "\n",
    "def encode_seq(seq):\n",
    "    return [WORD2IDX[word] for word in seq]\n",
    "\n",
    "def decode_seq(seq):\n",
    "    return [IDX2WORD[idx] for idx in seq]\n",
    "\n",
    "def decode_order(seq):\n",
    "    return [aug_idx-2 for aug_idx in seq]\n",
    "\n",
    "def decode_by_index(seq, idx_seq, end_idx, correction=-2): # -2: PAD and EOS\n",
    "    decoded = []\n",
    "    for idx in range(end_idx):\n",
    "        decoded.append(seq[idx_seq[idx]+correction])\n",
    "    return decoded\n",
    "\n",
    "def random_batch(batch_size, input_length_from=2, input_length_to=MAX_LEN-2):\n",
    "    if input_length_from >= input_length_to:\n",
    "        raise ValueError('length_from >= length_to')\n",
    "    input_lengths = np.random.randint(input_length_from, input_length_to, size=batch_size)\n",
    "    input_batch, output_batch = [], []\n",
    "    for length in input_lengths:\n",
    "        input_seq, output_seq = random_datum(length)\n",
    "        input_batch.append(encode_seq(input_seq))\n",
    "        output_batch.append(output_seq)\n",
    "    return input_batch, output_batch    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '3', '0', '8', '6', '1', '2', '9', '7', '4']\n",
      "[2, 5, 6, 1, 9, 0, 4, 8, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "a,b = random_datum(10)\n",
    "print a\n",
    "print decode_order(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 4], [11, 2, 6, 9, 3]]\n",
      "\n",
      "[[1, 0], [0, 1, 4, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "a, b = random_batch(batch_size=2)\n",
    "print a\n",
    "print\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7', '3']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 8, 11],\n",
       "        [ 4,  2],\n",
       "        [11,  6],\n",
       "        [11,  9],\n",
       "        [11,  3],\n",
       "        [11, 11],\n",
       "        [11, 11],\n",
       "        [11, 11],\n",
       "        [11, 11],\n",
       "        [11, 11],\n",
       "        [11, 11],\n",
       "        [11, 11]], dtype=int32), [2, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.batch(a, max_sequence_length=MAX_LEN, custom_pad=MAX_LEN-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Graph\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "vocab_size = len(VOCAB)\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2 # because encoder is going to be bidirectional.\n",
    "\n",
    "#                    decoder \n",
    "#                    target\n",
    "# \n",
    "# [] -> [] -> [#] -> [] -> []\n",
    "#                     |    ^\n",
    "# encoder             |____|\n",
    "# inputs             \n",
    "\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs') # [max_time, batch_size]\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length') \n",
    "    # this takes a vector (length=batch_size), where each cell is the length of the\n",
    "    # correponding data entry. this doesn't affect time_major op.\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs) # [max_time, batch_size, emb_size]\n",
    "\n",
    "encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "((encoder_fw_outputs,encoder_bw_outputs), # both have [max_time, batch_size, emb_size]\n",
    " (encoder_fw_final_state,encoder_bw_final_state)) = ( # state tuples: (c=[batch_size,emb_size],h=same)\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                        cell_bw=encoder_cell,\n",
    "                                        inputs=encoder_inputs_embedded,\n",
    "                                        sequence_length=encoder_inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "\n",
    "# concat fw-bw separately, then make a combined final state!\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2) # concat on emb dim.\n",
    "encoder_final_state_c = tf.concat((encoder_fw_final_state.c, encoder_bw_final_state.c), 1) # same thing.\n",
    "encoder_final_state_h = tf.concat((encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")\n",
    "\n",
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
    "    # getting the shape of a tensor [max_time, batch_size].\n",
    "    # doc: Unpacks the given dimension of a rank-`R` tensor into rank-`(R-1)` tensors.\n",
    "    # WHY: ?dynamically keeping track of the shape?\n",
    "decoder_lengths = encoder_inputs_length + 3 # +2 steps, +1 for EOS.\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32) # for dec only!\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n",
    "    # shared weights in the dynamic unrolling of the decoder.\n",
    "    # W shape: [emb_concat, vocab]\n",
    "    # it will be matmuled in output * W: [batch_size, emb_concat] * [emb_concat, vocab]\n",
    "    # get: [batch_size, vocab], where we have allthe predictions (as multinomials)\n",
    "\n",
    "# prepare tokens for each time step\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS') # [batch_size]\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice) # [max_time, batch_size]\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)\n",
    "\n",
    "# Loop feed (doc: tf.nn.raw_rnn?)\n",
    "# (time, prev_cell_output, prev_cell_state, prev_loop_state) ->\n",
    "# (elem_finished, input, cell_state, output, loop_state)\n",
    "\n",
    "# handles first state (i.e. corresponds to the last state of the encoder)\n",
    "#\n",
    "#     state feed only (enc final state)\n",
    "#         |\n",
    "#         v\n",
    "#      # --> #\n",
    "#   last     first \n",
    "#   of enc   of dec\n",
    "#            ^\n",
    "#            |\n",
    "#           EOS\n",
    "#\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths) # all false (i.e. not done) at the init step.\n",
    "    initial_input = eos_step_embedded                  # it's a [batch_size] length boolean vector.\n",
    "        # \"input\": it's the input for the next state.\n",
    "        # in this case, the first cell of the decoder.\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None # these two None help us\n",
    "    initial_loop_state = None  # checking whether we are at the init step.\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)\n",
    "\n",
    "# handles the transitions in decoder after the first state\n",
    "#             ___\n",
    "#  output ->  |  |\n",
    "#             # -|-> #\n",
    "#              / |   ^\n",
    "#         state  |___| <- next_input (inpt)\n",
    "#\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "    def get_next_input():\n",
    "        # at the first cell of the decoder, we take the feed from \n",
    "        # the final state of the encoder (handled by loop_fn_init),\n",
    "        # feed = EOS embedding\n",
    "        # and compute the first prediction. \n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "            # output * W: [batch_size, emb_concat] * [emb_concat, vocab]\n",
    "            # get: [batch_size, vocab], where we have all the predictions (as multinomials)\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    elements_finished = (time >= decoder_lengths) # again a [batch_size] boolean vector.\n",
    "        # this returns a boolean tensor, e.g. [1, 1, 1, 0]\n",
    "        # this means the first three steps are done, but not the last.\n",
    "        # when all the steps are done, i.e. time (the real time) is larger than\n",
    "        # the specified max decoding steps, the vector is all 1.\n",
    "        # then the next line will return 1.\n",
    "    finished = tf.reduce_all(elements_finished) # maps to boolean scalar.\n",
    "    inpt = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "        # if finished, return a pad for next input (i.e. the feed to next step)\n",
    "        # otherwise, return get_next_input as usual.\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "    # outputs:\n",
    "    # elements_finished: a [batch_size] boolean vector.\n",
    "    # inpt: [batch_size, emb_size] tensor for the next cell.\n",
    "    # state: (c,h) tuole, raw_rnn takes care of it.\n",
    "    # output: stored [batch_size, emb_size] tensor.\n",
    "    # loop_state: rnn_raw takes care of it.\n",
    "    return (elements_finished,\n",
    "            inpt, \n",
    "            state,\n",
    "            output,\n",
    "            loop_state)\n",
    "\n",
    "# combine the two fns above for a single looping function.\n",
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    # time: an int32 scalar raw_rnn uses to keep track of time-steps internally.\n",
    "    # previous_output: [max_time, batch_size, emb_size] tensor.\n",
    "    # previous_state: (c,h) tuple.\n",
    "    # previous_loop_state: raw_rnn uses to keep track of where it is in the loop (automatic).\n",
    "    if previous_state is None: # time = 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn) # we have an LSTM cell.\n",
    "    # *_ta: the RNN output (TensorArray <- for dynamic use)\n",
    "    # *_final_state: 2-tuple of [batch_size, emb_size] (i.e. c and h). of no use for seq2seq.\n",
    "    # _: final_loop_state, which no one gives a fuck (used internally by *.raw_rnn backend).\n",
    "decoder_outputs = decoder_outputs_ta.stack() # [max_time, batch_size, emb_concat]\n",
    "\n",
    "decoder_max_step, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "    # for matmul, we do\n",
    "    # [max_time, batch_size, emb_concat], [max_time*batch_size, emb_concat]\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_step, decoder_batch_size, vocab_size))\n",
    "    # put it back into the original shaping scheme.\n",
    "decoder_prediction = tf.cast(tf.argmax(decoder_logits, 2), dtype=tf.int32) # [max_time, batch_size]\n",
    "\n",
    "# [SU] report accuracy here\n",
    "correct_raw = tf.cast(tf.equal(decoder_prediction, decoder_targets), tf.int32)\n",
    "mask = tf.cast(tf.not_equal(decoder_targets, WORD2IDX['PAD']), tf.int32) # EOSs are not 0ed out, there are BATCH_SIZE of them.\n",
    "total_seqlen = tf.cast(tf.reduce_sum(encoder_inputs_length), tf.float32)\n",
    "correct = tf.multiply(correct_raw, mask)\n",
    "accuracy = tf.cast(tf.reduce_sum(correct)-BATCH_SIZE, tf.float32) / total_seqlen # -BATCH_SIZE correction\n",
    "# accuracy = tf.cast(tf.reduce_sum(correct), tf.float32) / total_seqlen # w/o the correction.\n",
    "\n",
    "# Optimization\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits\n",
    ")\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "  minibatch loss: 0.16543418169 | accuracy 0.846153855324\n",
      "  sample 1:\n",
      "    input     > ['6', '2', '8', '9', '1', '0', '3', '5', '7']\n",
      "    predicted > ['1', '2', '0', '5', '7', '6', '7', '8', '9']\n",
      "    target    > ['0', '1', '2', '3', '5', '6', '7', '8', '9']\n",
      "  sample 2:\n",
      "    input     > ['4', '1', '9', '3', '8', '5', '7', 'PAD', 'PAD']\n",
      "    predicted > ['1', '3', '4', '5', '7', '8', '9']\n",
      "    target    > ['1', '3', '4', '5', '7', '8', '9']\n",
      "  sample 3:\n",
      "    input     > ['5', '9', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['5', '9']\n",
      "    target    > ['5', '9']\n",
      "\n",
      "Batch 1000\n",
      "  minibatch loss: 0.144738689065 | accuracy 0.905660390854\n",
      "  sample 1:\n",
      "    input     > ['6', '4', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['4', '6']\n",
      "    target    > ['4', '6']\n",
      "  sample 2:\n",
      "    input     > ['3', '2', '6', '9', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['2', '3', '6', '9']\n",
      "    target    > ['2', '3', '6', '9']\n",
      "  sample 3:\n",
      "    input     > ['8', '2', '7', '4', '5', '3', '9', 'PAD']\n",
      "    predicted > ['2', '3', '4', '5', '7', '8', '9']\n",
      "    target    > ['2', '3', '4', '5', '7', '8', '9']\n",
      "\n",
      "Batch 2000\n",
      "  minibatch loss: 0.335499376059 | accuracy 0.805194795132\n",
      "  sample 1:\n",
      "    input     > ['7', '1', '4', '3', '9', '6', '8', 'PAD', 'PAD']\n",
      "    predicted > ['1', '3', '4', '6', '7', '8', '9']\n",
      "    target    > ['1', '3', '4', '6', '7', '8', '9']\n",
      "  sample 2:\n",
      "    input     > ['2', '0', '7', '3', '4', '8', '5', '6', '1']\n",
      "    predicted > ['0', '1', '2', '3', '4', '4', '8', '7', '8']\n",
      "    target    > ['0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
      "  sample 3:\n",
      "    input     > ['3', '9', '8', '7', '4', '2', '5', '1', '6']\n",
      "    predicted > ['1', '2', '3', '4', '1', '7', '6', '8', '9']\n",
      "    target    > ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "\n",
      "Batch 3000\n",
      "  minibatch loss: 0.0845424681902 | accuracy 0.959999978542\n",
      "  sample 1:\n",
      "    input     > ['5', '2', '7', '3', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['2', '3', '5', '7']\n",
      "    target    > ['2', '3', '5', '7']\n",
      "  sample 2:\n",
      "    input     > ['4', '8', '3', '7', '0', '6', '5', '2', 'PAD']\n",
      "    predicted > ['0', '5', '3', '4', '5', '6', '7', '8']\n",
      "    target    > ['0', '2', '3', '4', '5', '6', '7', '8']\n",
      "  sample 3:\n",
      "    input     > ['9', '6', '8', '0', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['0', '6', '8', '9']\n",
      "    target    > ['0', '6', '8', '9']\n",
      "\n",
      "Evaluation results (on 100 batches):\n",
      "  average loss: 0.14611646533 | average accuracy 0.896141827106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def next_feed(batch_size):\n",
    "    batch_enc, batch_dec = random_batch(batch_size)\n",
    "    encoder_inputs_, encoder_inputs_lengths_ = utils.batch(batch_enc)\n",
    "    decoder_targets_, _ = utils.batch([seq + [WORD2IDX['EOS']] + [WORD2IDX['PAD']]*2 for seq in batch_dec])\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_inputs_lengths_,\n",
    "        decoder_targets: decoder_targets_\n",
    "    }\n",
    "\n",
    "loss_track = []\n",
    "\n",
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "num_test_batches = 100\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed(BATCH_SIZE)\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('Batch {}'.format(batch))\n",
    "            print('  minibatch loss: {} | accuracy {}'.format(*sess.run([loss, accuracy], fd)))\n",
    "            # predict_ = sess.run(decoder_prediction, fd)\n",
    "            predict_, lengths_ = sess.run([decoder_prediction, encoder_inputs_length], fd) # make use of seqlen\n",
    "\n",
    "#             TEST BLOC\n",
    "#\n",
    "#             print fd[encoder_inputs].shape\n",
    "#             print predict_.shape\n",
    "#             print fd[decoder_targets].shape\n",
    "#             print\n",
    "#\n",
    "#             print 'seqlen:'\n",
    "#             print fd[encoder_inputs_length]\n",
    "#             print 'prediction:'\n",
    "#             print predict_\n",
    "#             print 'true:'\n",
    "#             print fd[decoder_targets]\n",
    "#             print 'mask:'\n",
    "#             print sess.run(mask, fd)\n",
    "#             print 'correct-raw:'\n",
    "#             print sess.run(correct_raw, fd)\n",
    "#             print 'correct:'\n",
    "#             print sess.run(correct, fd)\n",
    "#             print 'total_seqlen:'\n",
    "#             print sess.run(total_seqlen, fd)\n",
    "#             print 'accuracy:'\n",
    "#             print sess.run(accuracy, fd)\n",
    "#             assert 1==0\n",
    "            \n",
    "            for i, (inp, pred, tar, length) in enumerate(zip(fd[encoder_inputs].T, predict_.T, fd[decoder_targets].T, lengths_)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                decoded_inp = decode_seq(inp)\n",
    "                print('    input     > {}'.format(decoded_inp))\n",
    "                print('    predicted > {}'.format(decode_by_index(decoded_inp, pred, end_idx=length)))\n",
    "                print('    target    > {}'.format(decode_by_index(decoded_inp, tar, end_idx=length)))\n",
    "#                 print('    predicted > {}'.format(decode_order(pred))) # these print out raw indices\n",
    "#                 print('    target    > {}'.format(decode_order(tar)))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print\n",
    "            \n",
    "    # EVALUATE ON A BIG TEST SET\n",
    "    loss_track, accuracy_track = [], []\n",
    "    for _ in range(num_test_batches):\n",
    "        fd = next_feed(BATCH_SIZE)\n",
    "        l, a = sess.run([loss, accuracy], fd)\n",
    "        loss_track.append(l)\n",
    "        accuracy_track.append(a)\n",
    "    print('Evaluation results (on {} batches):'.format(num_test_batches))\n",
    "    print('  average loss: {} | average accuracy {}'.format(np.mean(loss_track), np.mean(accuracy_track)))\n",
    "    print\n",
    "    \n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "<type 'list'>\n",
      "['3', '9', '7', '5', '1'] [6, 2, 5, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "a,b = random_datum(5)\n",
    "print type(a)\n",
    "print type(b)\n",
    "print a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '3', '5', '7', '9']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_by_index(a,b,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3 5 7 9\n"
     ]
    }
   ],
   "source": [
    "for b_ in b:\n",
    "    print a[b_-2], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3', '9', '7', '5', '1'], \n",
       "      dtype='|S1')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<tf.Tensor 'encoder_inputs:0' shape=(?, ?) dtype=int32>: array([[10,  9,  4, 11, 10],\n",
       "        [ 2,  7,  6,  7,  6],\n",
       "        [ 4,  2,  5,  6,  2],\n",
       "        [ 5, 10, 11,  5,  8],\n",
       "        [ 0,  3,  2,  8,  0]], dtype=int32),\n",
       " <tf.Tensor 'encoder_inputs_length:0' shape=(?,) dtype=int32>: [4, 5, 5, 5, 4],\n",
       " <tf.Tensor 'decoder_targets:0' shape=(?, ?) dtype=int32>: array([[3, 4, 5, 2, 4],\n",
       "        [4, 6, 6, 5, 3],\n",
       "        [5, 3, 2, 4, 5],\n",
       "        [2, 2, 4, 3, 2],\n",
       "        [1, 5, 3, 6, 1],\n",
       "        [0, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]], dtype=int32)}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_feed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 9, 8], [9, 6, 8], [10, 2, 6, 8, 7, 3, 9, 11, 5], [9, 8, 11, 10, 5, 4, 6], [6, 8, 4, 11, 2, 9, 5, 10, 3]]\n",
      "[[0, 2, 1], [1, 2, 0], [7, 1, 5, 8, 2, 4, 3, 6, 0], [2, 5, 4, 6, 1, 0, 3], [3, 4, 8, 2, 6, 0, 1, 5, 7]]\n"
     ]
    }
   ],
   "source": [
    "a, b = random_batch(5)\n",
    "print a\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1,  7,  2,  3],\n",
       "        [ 2,  2,  1,  5,  4],\n",
       "        [ 1,  0,  5,  4,  8],\n",
       "        [ 1,  1,  8,  6,  2],\n",
       "        [ 0,  0,  2,  1,  6],\n",
       "        [ 0,  0,  4,  0,  0],\n",
       "        [11, 11,  3,  3,  1],\n",
       "        [11, 11,  6,  1,  5],\n",
       "        [11, 11,  0,  0,  7],\n",
       "        [11, 11,  1,  0,  1],\n",
       "        [11, 11,  0, 11,  0],\n",
       "        [11, 11,  0, 11,  0]], dtype=int32), [6, 6, 12, 10, 12])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.batch([seq + [WORD2IDX['EOS']] + [WORD2IDX['PAD']]*2 for seq in b], custom_pad=MAX_LEN-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1,  7,  2,  3],\n",
       "        [ 2,  2,  1,  5,  4],\n",
       "        [ 1,  0,  5,  4,  8],\n",
       "        [11, 11,  8,  6,  2],\n",
       "        [11, 11,  2,  1,  6],\n",
       "        [11, 11,  4,  0,  0],\n",
       "        [11, 11,  3,  3,  1],\n",
       "        [11, 11,  6, 11,  5],\n",
       "        [11, 11,  0, 11,  7]], dtype=int32), [3, 3, 9, 7, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.batch(b, custom_pad=MAX_LEN-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def random_batch(batch_size, input_length_from=2, input_length_to=MAX_LEN-2):\n",
    "    if input_length_from >= input_length_to:\n",
    "        raise ValueError('length_from >= length_to')\n",
    "    input_lengths = np.random.randint(input_length_from, input_length_to, size=batch_size)\n",
    "    input_batch, output_batch = [], []\n",
    "    for length in input_lengths:\n",
    "        input_seq, output_seq = random_datum(length)\n",
    "        input_batch.append(encode_seq(input_seq))\n",
    "        output_batch.append(output_seq)\n",
    "    return input_batch, output_batch    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
