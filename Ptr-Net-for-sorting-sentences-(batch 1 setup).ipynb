{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add custom import path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jacobsuwang/Documents/UTA2018/NEURAL-NETS/ATTENTION/CODE/01-import-folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "README\n",
      "\n",
      "@ 'tar-order':targets_orders\n",
      "\n",
      "List of sentence order lists; each list = integers indexing permuted sentences in the doc.\n",
      "\n",
      "@ 'inp-encode':inputs_encoded\n",
      "\n",
      "List of documents; each doc = a list of sentences; each sent = a list of word indices.\n",
      "\n",
      "@ 'inp-slen':inputs_sent_lengths\n",
      "\n",
      "List of length info of documents; each info = a list of sentence lengths.\n",
      "\n",
      "@ 'w-indexer':word_indexer\n",
      "\n",
      "Indexer() class. word <-> word index.\n",
      "\n",
      "@ 'idx-emb':idx2emb\n",
      "\n",
      "dict() class. index <-> glove embeddings.\n",
      "\n",
      "@ 'glove-init':glove_init\n",
      "\n",
      "Initializer of embedding matrix. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import dill\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "bl08_path = '/home/jacobsuwang/Documents/UTA2018/NEURAL-NETS/ATTENTION/DATA/COHERENCE/data1-train-encoded-data.p'\n",
    "\n",
    "data_dict = dill.load(open(bl08_path, 'rb'))\n",
    "\n",
    "print data_dict['readme']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_DOC: 26 | MAX_SENT: 61\n"
     ]
    }
   ],
   "source": [
    "# Padding inputs\n",
    "\n",
    "MAX_SENT = 61\n",
    "MAX_DOC = 26\n",
    "MAX_SENT, MAX_DOC = 0, 0\n",
    "for doc in data_dict['inp-encode']:\n",
    "    if len(doc)>MAX_DOC:\n",
    "        MAX_DOC = len(doc)\n",
    "    for sent in doc:\n",
    "        if len(sent)>MAX_SENT:\n",
    "            MAX_SENT = len(sent)\n",
    "print 'MAX_DOC: {} | MAX_SENT: {}'.format(MAX_DOC, MAX_SENT)\n",
    "\n",
    "def pad_doc(doc):\n",
    "    padded_doc = []\n",
    "    doc_length = len(doc)\n",
    "    for sent in doc:\n",
    "        padded_sent = sent + [0]*(MAX_SENT-len(sent))\n",
    "        padded_doc.append(padded_sent)\n",
    "    padded_doc += [[0]*MAX_SENT]*(MAX_DOC-doc_length)\n",
    "    return padded_doc, doc_length\n",
    "\n",
    "def pad_sents_length(slen):\n",
    "    return slen + [0]*(MAX_DOC-len(slen))\n",
    "\n",
    "def pad_tars_order(tord):\n",
    "    return tord + [0]*3 # +1 EOS, +2 PADs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "## CONFIGS ##\n",
    "\n",
    "sent_emb_size = 20\n",
    "doc_enc_emb_size = sent_emb_size*2\n",
    "doc_dec_emb_size = doc_enc_emb_size*2\n",
    "    # the model is sent-enc (bilstm) -> doc-enc (bilstm) -> doc-dec (unilstm)\n",
    "    # so the #dim doubles twice through the two bilstms.\n",
    "\n",
    "sent_vocab_size = MAX_DOC + 2\n",
    "    # #sents in longest doc + PAD + EOS.\n",
    "    # no known better solution, because the final weight matrix \n",
    "    # W = [dec-emb, pred-dim] must be fixed.\n",
    "    \n",
    "lr = 1e-5\n",
    "    \n",
    "## INPUT PORT ##\n",
    "    \n",
    "doc_inputs = tf.placeholder(tf.int32, shape=[MAX_DOC, MAX_SENT], name='doc-inputs')\n",
    "    # 1 document padded to the longest of docs and the longest of sents.\n",
    "    # NB: here it is treated as [batch_size=MAX_DOC, max_time=MAX_SENT].\n",
    "doc_sents_length = tf.placeholder(tf.int32, shape=[MAX_DOC], name='doc-sents-length')\n",
    "    # lengths of sentences in the doc.\n",
    "\n",
    "embeddings = tf.get_variable('word-embeddings', shape=data_dict['glove-init'].shape, # (1809,300)\n",
    "                             initializer=tf.constant_initializer())\n",
    "glove_feed = tf.placeholder(tf.float32, shape=data_dict['glove-init'].shape)\n",
    "glove_init = embeddings.assign(glove_feed)\n",
    "    # above initializes the embedding matrix [vocab_size, emb_size]\n",
    "    # with pretrained 300D GloVe vectors.\n",
    "doc_inputs_embedded = tf.transpose(tf.nn.embedding_lookup(embeddings, doc_inputs),[1,0,2]) \n",
    "    # transpose to [max_time, batch_size, emb_size] for time-major.\n",
    "\n",
    "## SENTENCE ENCODER ##\n",
    "    \n",
    "with tf.variable_scope('sent-enc'):\n",
    "    sent_enc_cell = LSTMCell(sent_emb_size)\n",
    "    ((sent_enc_fw_outputs,sent_enc_bw_outputs), \n",
    "     (sent_enc_fw_final_state,sent_enc_bw_final_state)) = ( \n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=sent_enc_cell,\n",
    "                                            cell_bw=sent_enc_cell,\n",
    "                                            inputs=doc_inputs_embedded,\n",
    "                                            sequence_length=doc_sents_length,\n",
    "                                            dtype=tf.float32, time_major=True)\n",
    "        )\n",
    "    sent_enc_outputs = tf.concat((sent_enc_fw_outputs,sent_enc_bw_outputs), 2)\n",
    "        # [max_time, batch_size, emb_size].\n",
    "    sent_enc_final_state_c = tf.concat((sent_enc_fw_final_state.c,sent_enc_bw_final_state.c), 1)\n",
    "    sent_enc_final_state_h = tf.concat((sent_enc_fw_final_state.c,sent_enc_bw_final_state.h), 1)\n",
    "    sent_enc_final_state = LSTMStateTuple(\n",
    "        c=sent_enc_final_state_c, # both c & h are [batch_size, emb_size].\n",
    "        h=sent_enc_final_state_h  # basically a list of sentence embeddings of the doc.\n",
    "    )    \n",
    "\n",
    "doc_final_embedded = tf.expand_dims(sent_enc_final_state.h, 1)\n",
    "    # now the original \"batch_size\" is reinterpreted as the max_time/length \n",
    "    # of the document (which maxes at MAX_DOC).\n",
    "    # thus, we add a batch dimension at dim-1 to maintain the\n",
    "    # [max_time, batch_size, emb_size] time-major form for convenience of processing.\n",
    "doc_length = tf.placeholder(tf.int32, shape=[1], name='doc-length')\n",
    "    # 1 scalar that will be fed in as [*], document length.\n",
    "doc_targets = tf.placeholder(tf.int32, shape=[None, 1], name='doc-targets') \n",
    "    # the correct order indices for the jumbled document.\n",
    "    # [max_time=#sents in doc, batch_size=1]\n",
    "\n",
    "## DOCUMENT ENCODER ##\n",
    "\n",
    "with tf.variable_scope('doc-enc'):\n",
    "    doc_enc_cell = LSTMCell(doc_enc_emb_size)\n",
    "    ((doc_enc_fw_outputs,doc_enc_bw_outputs), \n",
    "     (doc_enc_fw_final_state,doc_enc_bw_final_state)) = ( \n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=doc_enc_cell,\n",
    "                                            cell_bw=doc_enc_cell,\n",
    "                                            inputs=doc_final_embedded,\n",
    "                                            sequence_length=doc_length,\n",
    "                                            dtype=tf.float32, time_major=True)\n",
    "        )    \n",
    "    doc_enc_outputs = tf.concat((doc_enc_fw_outputs,doc_enc_bw_outputs), 2) \n",
    "        # [max_time, batch_size, emb_size].\n",
    "    doc_enc_final_state_c = tf.concat((doc_enc_fw_final_state.c,doc_enc_bw_final_state.c), 1)\n",
    "    doc_enc_final_state_h = tf.concat((doc_enc_fw_final_state.c,doc_enc_bw_final_state.h), 1)\n",
    "    doc_enc_final_state = LSTMStateTuple(\n",
    "        c=doc_enc_final_state_c, # [batch_size, emb_size], where batch_size=1 always\n",
    "        h=doc_enc_final_state_h  # as only 1 document gets entered per time.\n",
    "    )\n",
    "    \n",
    "## DOCUMENT DECODER ##\n",
    "    \n",
    "doc_dec_cell = LSTMCell(doc_dec_emb_size)\n",
    "doc_max_time, doc_batch_size, _ = tf.unstack(tf.shape(doc_final_embedded))\n",
    "    # dynamically passing these dimensions from enc to dec.\n",
    "doc_dec_length = doc_length + 3 \n",
    "    # MAX_DOC +2 steps, +1 for EOS. \n",
    "\n",
    "W = tf.get_variable('W', [doc_dec_emb_size, sent_vocab_size], dtype=tf.float32,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable('b', [sent_vocab_size], dtype=tf.float32,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "    # the final softmax layer for the decoder,\n",
    "    # softmanx(W*out + b)\n",
    "\n",
    "eos_step_embedded = tf.ones([doc_batch_size, doc_enc_emb_size], dtype=tf.float32, name='EOS')\n",
    "pad_step_embedded = tf.zeros([doc_batch_size, doc_enc_emb_size], dtype=tf.float32, name='PAD')\n",
    "    # doc_batch_size=1, doc_enc_emb_size=(unilstm's dim in doc encoder).\n",
    "    # NB: even batch_size=1, must use this dynamic control for TF to interpret.\n",
    "\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= doc_dec_length) \n",
    "        # false along batch_size, i.e. none is done at this (init) step.\n",
    "    initial_input = eos_step_embedded                   \n",
    "    initial_cell_state = doc_enc_final_state\n",
    "    initial_cell_output = None \n",
    "    initial_loop_state = None \n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)\n",
    " \n",
    "W1 = tf.get_variable('W1', [doc_enc_emb_size, doc_enc_emb_size], dtype=tf.float32,\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2 = tf.get_variable('W2', [doc_dec_emb_size, doc_enc_emb_size], dtype=tf.float32,\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "v = tf.get_variable('v', [doc_enc_emb_size, 1], dtype=tf.float32,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "    # raw weights for computing \"the attention of decoding step i on the\n",
    "    # encoding step j\".\n",
    "    # u_j^i = v^T tanh(W1*e_j + W2*d_i) Vinyals et al. (2016, Section 2.3).\n",
    "\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "    \n",
    "    ## ATTENTION ##\n",
    "    \n",
    "    def get_next_input(): \n",
    "        mt, bc, _ = tf.unstack(tf.shape(doc_final_embedded))\n",
    "            # dynamically manage max_time and batch_size.\n",
    "        EW1 = tf.reshape(tf.tensordot(doc_final_embedded, W1, axes=[[2],[0]]),\n",
    "                         [mt, bc, doc_enc_emb_size]) \n",
    "            # [mt,bc,enc_emb] * [enc_emb,enc_emb] = [mt,bc,enc_emb]\n",
    "            # batch computing W1*e_j above.\n",
    "        DW2 = tf.matmul(previous_state.h, W2)\n",
    "            # [bc,dec_emb] * [dec_emb,enc_emb] = [bc,enc_emb]\n",
    "            # batch computing W2*d_i.\n",
    "        EW1_add_DW2 = tf.add(EW1, DW2)\n",
    "            # [mt,bc,enc_emb] + [bc,enc_emb] = [mt,bc,enc_emb]\n",
    "            # the identity in the last two dimensions invokes broadcasting.\n",
    "        attention_mat = tf.reshape(tf.nn.tanh(tf.squeeze(tf.tensordot(EW1_add_DW2, v, axes=[[2],[0]]), \n",
    "                                                         axis=2)), [mt,bc])\n",
    "            # op1. EW1_add_DW2 * v\n",
    "            #   [mt,bc,enc_emb] * [enc_emb, 1] = [mt,bc,1]\n",
    "            #   the vacuous 1 dim of v must be there to enable tensordot.\n",
    "            # op2. squeeze(res-op1)\n",
    "            #   remove dim-2, which = 1.\n",
    "            #   squeeze([mt,bc,1], 2) = [mt,bc]\n",
    "            # op3. reshape(res-op2)\n",
    "            #   explicitly specify the [mt,bc] shape for easy interpretation.\n",
    "            #   NB: mt,bc must be dynamically extracted dims (using unstack(shape))!\n",
    "        attention_norm_mat = tf.nn.softmax(attention_mat, dim=0) \n",
    "            # softmax along the max_time axis (#sentences in doc), i.e.\n",
    "            # the attention weights of all sentences sum up to 1.\n",
    "        selector = tf.one_hot(tf.argmax(attention_norm_mat, axis=0), depth=doc_max_time,\n",
    "                              on_value=1.0, off_value=0.0, axis=0) \n",
    "            # an [mt,bc] shaped array of one-hot vectors (i.e. a matrix).\n",
    "            # it picks out the argmax sentence embeddings for feeding to\n",
    "            # the next time step.\n",
    "        inputs_embedded_selected = tf.transpose(\n",
    "            tf.multiply(\n",
    "                tf.transpose(doc_final_embedded, [2,0,1]), \n",
    "                selector), \n",
    "            [1,2,0]\n",
    "        ) \n",
    "            # op1. transpose([mt,bc,enc_emb], [2,0,1])\n",
    "            #   [mt,bc,enc_emb] -> [enc_emb,mt,bc]\n",
    "            #   to allow for broadcast in below.\n",
    "            # op2. multiply([enc_emb,mt,bc], [mt,bc])\n",
    "            #   broadcast to 0-ify the non-max sentence embeddings.\n",
    "            # op3. transpose([enc_emb,mt,bc], [1,2,0])\n",
    "            #   [enc_emb,mt,bc] -> [mt,bc,enc_emb]\n",
    "            #   return to time-major.\n",
    "        inputs_embedded_selected = tf.reduce_sum(\n",
    "            tf.reshape(inputs_embedded_selected, [mt, bc, doc_enc_emb_size]), \n",
    "            axis=0 \n",
    "        ) \n",
    "            # op1. reshape\n",
    "            #   explicitly specify the [mt,bc,enc_emb] shape for easy interpretation.\n",
    "            # op2. reduce_sum(res-op1)\n",
    "            #   collapse the tensor along max_time to leave behind only\n",
    "            #   the max sentence embedding (the other embs are 0-fied now,\n",
    "            #   so nothing changes except for shape).\n",
    "        next_input = inputs_embedded_selected\n",
    "            # feed the resulting max sentence embedding to the next time step.\n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= doc_dec_length)\n",
    "        # if time >= doc_dec_length, it means we are done processing\n",
    "        # all the sentences in the doc.\n",
    "    finished = tf.reduce_all(elements_finished) \n",
    "        # when finished, this should be true.\n",
    "        # e.g. if x=[T,F,T], reduce_all(x)=F,\n",
    "        #      if x=[T,T,T], reduce_all(x)=T.\n",
    "        # the reduction is effectively along the batch dim \n",
    "        # (but configurable to be restricted to a selected dimension, or all).\n",
    "    inpt = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "        # cond(bool, pos-res, neg-res)\n",
    "        # if finished=True, return pos-res, i.e. ending state (PAD).\n",
    "        # otherwise return neg-res, i.e. the next input.\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "    return (elements_finished,\n",
    "            inpt, \n",
    "            state,\n",
    "            output,\n",
    "            loop_state)\n",
    "        # returns:\n",
    "        # elements_finished: a [batch_size] boolean vector.\n",
    "        # inpt: [batch_size, emb_size] tensor for the next time step.\n",
    "        # state: (c,h) tuple, raw_rnn takes care of it.\n",
    "        # output: stored [batch_size, emb_size] tensor.\n",
    "        # loop_state: rnn_raw takes care of it.\n",
    "\n",
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "        # time: an int32 scalar raw_rnn uses to keep track of time-steps internally.\n",
    "        # previous_output: [max_time, batch_size, emb_size] tensor.\n",
    "        # previous_state: (c,h) tuple.\n",
    "        # previous_loop_state: raw_rnn uses to keep track of where it is in the loop (automatic).\n",
    "    if previous_state is None:\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "    \n",
    "## PREDICTION ##\n",
    "\n",
    "doc_dec_outputs_ta, doc_dec_final_state, _ = tf.nn.raw_rnn(doc_dec_cell, loop_fn)\n",
    "doc_dec_outputs = doc_dec_outputs_ta.stack()\n",
    "    # doc_dec_outputs = [max_time=#sents in doc, batch_size=1, dec_emb_size]\n",
    "    # gather outputs (dynamic TensorArray to Tensor).\n",
    "doc_dec_max_step, doc_dec_batch_size, doc_dec_dim = tf.unstack(tf.shape(doc_dec_outputs))\n",
    "    # dynamically extract the mt,bc,emb of the particular data entry.\n",
    "doc_dec_outputs_flat = tf.reshape(doc_dec_outputs, (-1, doc_dec_dim))\n",
    "    # reshape to [mt*bc, emb] for the last, softmax layer.\n",
    "    # because there we do softmax(W*vector + b).\n",
    "doc_dec_logits_flat = tf.add(tf.matmul(doc_dec_outputs_flat, W), b)\n",
    "    # [mt*bc, dec_emb] * [dec_emb,output_dim=sent_vocab=MAX_DOC+EOS+PAD] + [output_dim]\n",
    "    #   = [mt*bc, output_dim]\n",
    "doc_dec_logits = tf.reshape(doc_dec_logits_flat, (doc_dec_max_step, doc_dec_batch_size, sent_vocab_size))\n",
    "    # reshape to [mt,bc,output_dim] to get prediction.\n",
    "doc_dec_prediction = tf.cast(tf.argmax(doc_dec_logits, 2), dtype=tf.int32)\n",
    "    # argmax long dim=output_dim to get the max-dim in output_dim=MAX_DOC+EOS+PAD.\n",
    "\n",
    "## EVALUATION (ACCURACY) ##\n",
    "\n",
    "correct_raw = tf.cast(tf.equal(doc_dec_prediction, doc_targets), tf.int32)\n",
    "    # equal(pred=[mt=#sents in doc,bc=1], true=[mt=#sents in doc,bc=1])\n",
    "    #   = [mt,bc]\n",
    "mask = tf.cast(tf.not_equal(doc_targets, 0), tf.int32)\n",
    "    # mark non-zero entries in doc_targets with 1.\n",
    "    # [mt,bc] shaped.\n",
    "total_seqlen = tf.cast(doc_length, tf.float32)\n",
    "    # #sents in doc. float to get float accuracy value.\n",
    "correct = tf.multiply(correct_raw, mask)\n",
    "    # [mt,bc] * [mt,bc], where if correct, get correct=1*mask=1 = 1\n",
    "    # otherwise get correct=1*mask=0 = 0.\n",
    "    # get [mt,bc] matrix here.\n",
    "accuracy = tf.cast(tf.reduce_sum(correct)-1, tf.float32) / total_seqlen\n",
    "    # reduce_sum(correct=[mt,bc]) / total_seqlen = scalar / [seqlen]\n",
    "    #   = [accuracy]. doesn't matter because doesn't affect np.mean(accuracies).\n",
    "    \n",
    "## OPTIMIZATION ##\n",
    "\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(doc_targets, depth=sent_vocab_size, dtype=tf.float32),\n",
    "    logits=doc_dec_logits\n",
    ")\n",
    "    # labels=one_hot(doc_targets=[mt,bc],output_dim=sent_vocab)=[mt,bc,output_dim]\n",
    "    # logits=[mt,bc,output_dim]\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ROUGH) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_track = []\n",
    "accuracy_track = []\n",
    "\n",
    "num_epochs = 10\n",
    "verbose = 100\n",
    "\n",
    "sess.run(glove_feed, feed_dict={glove_feed:data_dict['glove-init']})\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    print 'Epoch {}:'.format(e+1)\n",
    "    print\n",
    "    for i in range(len(data_dict['inp-encode'])):\n",
    "        doc_inputs_, _ = pad_doc(data_dict['inp-encode'][i])\n",
    "        doc_sents_length_ = pad_sents_length(data_dict['inp-slen'][i])\n",
    "        doc_length_ = [len(data_dict['inp-encode'][i])] # NB: must be [2]!!!\n",
    "        doc_targets_ = np.array(pad_tars_order(data_dict['tar-order'][i]))[:,np.newaxis]\n",
    "        fd = {doc_inputs:doc_inputs_, \n",
    "              doc_sents_length:doc_sents_length_,\n",
    "              doc_length:doc_length_, \n",
    "              doc_targets:doc_targets_}\n",
    "        _, l, a = sess.run([train_op, loss, accuracy], feed_dict=fd)\n",
    "        loss_track.append(l)\n",
    "        accuracy_track.append(a)\n",
    "        if i % verbose == 0:\n",
    "            print 'Current mean loss = {} | mean accuracy = {}'.format(np.mean(loss_track),np.mean(accuracy_track))\n",
    "    print\n",
    "\n",
    "# Results with 0 tuning\n",
    "#\n",
    "# Epoch 100:\n",
    "\n",
    "# Current mean loss = 0.301753968 | mean accuracy = 0.758796215057\n",
    "# Current mean loss = 0.301661282778 | mean accuracy = 0.758872568607\n",
    "# Current mean loss = 0.301440030336 | mean accuracy = 0.758985757828\n",
    "# Current mean loss = 0.300857931376 | mean accuracy = 0.759163379669\n",
    "# Current mean loss = 0.300510913134 | mean accuracy = 0.759310424328"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
