{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ptr-Net \n",
    "\n",
    "* Vinyals et al. (2016)\n",
    "* Task: sorting digit strings (vocab=range(0,10))\n",
    "    * Input: ['2','3','1']\n",
    "    * Output: ['1','2','3']\n",
    "\n",
    "\n",
    "* Why: this is for when the outputs of the decoder are directly related to the inputs. E.g. in string sorting, the outputs' length should equal to that of the inputs. Thus, the model should only predicts a distribution over input strings, unlike regular enc-dec, which predicts distributions over a fixed vocabulary. This is not a problem for word sorting, for instance, but it is for sentence sorting, because there isn't a fixed vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add custom import path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jacobsuwang/Documents/UTA2018/NEURAL-NETS/ATTENTION/CODE/01-import-folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import utils # utils.batch produces time-major inputs (examples below).\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "VOCAB = set(['PAD','EOS','1','2','3','4','5','6','7','8','9','0'])\n",
    "NUMBERS = ['1','2','3','4','5','6','7','8','9','0']\n",
    "MAX_LEN = len(NUMBERS)\n",
    "WORD2IDX = {'PAD':0,'EOS':1,'1':2,'2':3,'3':4,'4':5,'5':6,'6':7,'7':8,'8':9,'9':10,'0':11}\n",
    "IDX2WORD = {idx:word for word,idx in WORD2IDX.iteritems()}\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "def random_datum(length):\n",
    "    \"\"\"\n",
    "    Takes a length int, returns an encoded (input-sequence, output-sequence) tuple.\n",
    "    \"\"\"\n",
    "    input_seq = np.random.choice(NUMBERS, length, replace=False)\n",
    "    output_seq = sorted(input_seq)\n",
    "    input_seq = map(lambda w:WORD2IDX[w], input_seq)\n",
    "    output_seq = map(lambda w:WORD2IDX[w], output_seq)\n",
    "    return input_seq, output_seq\n",
    "\n",
    "def random_batch(batch_size, length_from=2, length_to=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Takes a batch size, and the length range for generated entries,\n",
    "    returns an encoded (input-batch, output-batch), where each batch\n",
    "    is a list of input-sequence/output-sequence.\n",
    "    \"\"\"\n",
    "    if length_from >= length_to:\n",
    "        raise ValueError('length_from must be strictly smaller than length_to')\n",
    "    lengths = np.random.randint(length_from, length_to, size=batch_size)\n",
    "    input_batch, output_batch = [], []\n",
    "    for length in lengths:\n",
    "        input_seq, output_seq = random_datum(length)\n",
    "        input_batch.append(input_seq)\n",
    "        output_batch.append(output_seq)\n",
    "    return input_batch, output_batch\n",
    "\n",
    "def decode_seq(seq):\n",
    "    \"\"\"\n",
    "    Code to digit-string translation.\n",
    "    \"\"\"\n",
    "    return map(lambda idx:IDX2WORD[idx], seq)\n",
    "\n",
    "def decode_batch(batch):\n",
    "    \"\"\"\n",
    "    Digit-string to code translation.\n",
    "    \"\"\"\n",
    "    return map(lambda seq:decode_seq(seq), batch)\n",
    "\n",
    "# Example:\n",
    "# >> a, b = random_batch(3)\n",
    "# >> print a\n",
    "# >> print b\n",
    "# [[10, 2], [6, 8, 10, 5, 2, 3, 7, 4, 9], [10, 7, 6, 8, 3, 11, 4, 5, 9]]\n",
    "# [[2, 10], [2, 3, 4, 5, 6, 7, 8, 9, 10], [11, 3, 4, 5, 6, 7, 8, 9, 10]]\n",
    "# >> print decode_batch(a)\n",
    "# [['9', '1'],\n",
    "#  ['5', '7', '9', '4', '1', '2', '6', '3', '8'],\n",
    "#  ['9', '6', '5', '7', '2', '0', '3', '4', '8']]\n",
    "# >> decode_batch(b)\n",
    "# [['1', '9'],\n",
    "#  ['1', '2', '3', '4', '5', '6', '7', '8', '9'],\n",
    "#  ['0', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "# >> utils.batch(a)\n",
    "# (array([[10,  6, 10],\n",
    "#         [ 2,  8,  7],\n",
    "#         [ 0, 10,  6],\n",
    "#         [ 0,  5,  8],\n",
    "#         [ 0,  2,  3],\n",
    "#         [ 0,  3, 11],\n",
    "#         [ 0,  7,  4],\n",
    "#         [ 0,  4,  5],\n",
    "#         [ 0,  9,  9]], dtype=int32), [2, 9, 9])\n",
    "# >> utils.batch(b)\n",
    "# (array([[ 2,  2, 11],\n",
    "#         [10,  3,  3],\n",
    "#         [ 0,  4,  4],\n",
    "#         [ 0,  5,  5],\n",
    "#         [ 0,  6,  6],\n",
    "#         [ 0,  7,  7],\n",
    "#         [ 0,  8,  8],\n",
    "#         [ 0,  9,  9],\n",
    "#         [ 0, 10, 10]], dtype=int32), [2, 9, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "vocab_size = len(VOCAB)\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2\n",
    "\n",
    "#                    decoder \n",
    "#                    target\n",
    "# \n",
    "# [] -> [] -> [#] -> [] -> []\n",
    "#                     |    ^\n",
    "# encoder             |    |    \n",
    "# inputs              |    |        \"Fish-hook\" inputs to next step:\n",
    "#    ^                v    |        u_j^i = v^T tanh(W1*e_j + W2*d_i) (Section 2.3)\n",
    "#    |_____________ attention   <=  next_input = softmax(u^i)\n",
    "#    \n",
    "\n",
    "# Inputs\n",
    "#\n",
    "#   encoder_inputs: [max_time, batch_size]\n",
    "#   encoder_inputs: [batch_size]\n",
    "#   decoder_targets: [max_time, batch_size]\n",
    "#\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs') \n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length') \n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "# Embeddings\n",
    "#\n",
    "#   embeddings: [vocab_size, emb_size]\n",
    "#   encoder_inputs_embedded: [max_time, batch_size, emb_size]\n",
    "#\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "\n",
    "encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "((encoder_fw_outputs,encoder_bw_outputs), \n",
    " (encoder_fw_final_state,encoder_bw_final_state)) = ( \n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                        cell_bw=encoder_cell,\n",
    "                                        inputs=encoder_inputs_embedded,\n",
    "                                        sequence_length=encoder_inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2) # concat on emb dim.\n",
    "encoder_final_state_c = tf.concat((encoder_fw_final_state.c, encoder_bw_final_state.c), 1) # same thing.\n",
    "encoder_final_state_h = tf.concat((encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")\n",
    "\n",
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs)) # use runtime shape even w/ None.\n",
    "decoder_lengths = encoder_inputs_length + 3 # +2 steps, +1 for EOS.\n",
    "\n",
    "# Weights for decoder to make predictions\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32) # for dec only!\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n",
    "\n",
    "# EOS and PAD\n",
    "#\n",
    "#    eos_step_embedded: initializing the first input to the decoder.\n",
    "#    pad_step_embedded: the last \"prediction\" when the input length is exhausted. \n",
    "#\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS') \n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice) \n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)\n",
    "\n",
    "# Loop initializer\n",
    "#\n",
    "#   handles the transitions in decoder after the first state\n",
    "#              ___\n",
    "#   output ->  |  |\n",
    "#              # -|-> #\n",
    "#               / |   ^\n",
    "#          state  |   | <- next_input (inpt)\n",
    "#   inp_embs_____attention\n",
    "#\n",
    "\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths) # all false (i.e. not done) at the init step.\n",
    "    initial_input = eos_step_embedded                  # it's a [batch_size] length boolean vector.   \n",
    "        # \"input\": it's the input for the next state.\n",
    "        # in this case, the first cell of the decoder.\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None \n",
    "    initial_loop_state = None \n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)\n",
    "\n",
    "# Weights for the Ptr-attention\n",
    "#\n",
    "#   u_j^i = v^T tanh(W1*e_j + W2*d_i) (Section 2.3)\n",
    "#\n",
    "W1 = tf.Variable(tf.random_uniform([encoder_hidden_units, encoder_hidden_units], -1, 1),\n",
    "                 dtype=tf.float32) \n",
    "W2 = tf.Variable(tf.random_uniform([decoder_hidden_units, encoder_hidden_units], -1, 1), \n",
    "                 dtype=tf.float32) \n",
    "v = tf.Variable(tf.random_uniform([encoder_hidden_units, 1], -1, 1),\n",
    "                dtype=tf.float32) \n",
    "\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "    \n",
    "    def get_next_input(): \n",
    "        mt, bc, _ = tf.unstack(tf.shape(encoder_inputs_embedded)) # get runtime shape of max_time, batch.\n",
    "        EW1 = tf.reshape(tf.tensordot(encoder_inputs_embedded, W1, axes=[[2],[0]]),\n",
    "                         [mt, bc, encoder_hidden_units]) # [max_time, batch_size, emb_size]\n",
    "        DW2 = tf.matmul(previous_state.h, W2) # [batch_size, emb_size]\n",
    "        EW1_add_DW2 = tf.add(EW1, DW2)\n",
    "        # v^T * tanh(W1*e_j + W2*d_i) in batch mode.\n",
    "        attention_mat = tf.reshape(tf.nn.tanh(tf.squeeze(tf.tensordot(EW1_add_DW2, v, axes=[[2],[0]]), \n",
    "                                                         axis=2)), [mt,bc]) # unnormalized attention mat, [mt, bc]\n",
    "        attention_norm_mat = tf.nn.softmax(attention_mat, dim=0) # cols sum to 1 now.\n",
    "        selector = tf.one_hot(tf.argmax(attention_norm_mat, axis=0), depth=encoder_max_time,\n",
    "                              on_value=1.0, off_value=0.0, axis=0) # selects max-attended input embs.\n",
    "        inputs_embedded_selected = tf.transpose(\n",
    "            tf.multiply(\n",
    "                tf.transpose(encoder_inputs_embedded, [2,0,1]), \n",
    "                selector), \n",
    "            [1,2,0]\n",
    "        ) # transposing: to allow for multiply broadcast.\n",
    "        inputs_embedded_selected = tf.reduce_sum(\n",
    "            tf.reshape(inputs_embedded_selected, [mt, bc, encoder_hidden_units]), # explicitly interpret dims.\n",
    "            axis=0 # compress max_time dimension to only output selected embeddings.\n",
    "        )   \n",
    "        next_input = inputs_embedded_selected\n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths)\n",
    "        # this returns a boolean tensor, e.g. [1, 1, 1, 0]\n",
    "        # this means the first three steps are done, but not the last.\n",
    "        # when all the steps are done, i.e. time (the real time) is larger than\n",
    "        # the specified max decoding steps, the vector is all 1.\n",
    "        # then the next line will return 1.    \n",
    "    finished = tf.reduce_all(elements_finished) \n",
    "    inpt = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "        # if finished, return a pad for next input (i.e. the feed to next step)\n",
    "        # otherwise, return get_next_input as usual.\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "    # outputs:\n",
    "    # elements_finished: a [batch_size] boolean vector.\n",
    "    # inpt: [batch_size, emb_size] tensor for the next cell.\n",
    "    # state: (c,h) tuple, raw_rnn takes care of it.\n",
    "    # output: stored [batch_size, emb_size] tensor.\n",
    "    # loop_state: rnn_raw takes care of it.\n",
    "    return (elements_finished,\n",
    "            inpt, \n",
    "            state,\n",
    "            output,\n",
    "            loop_state)  \n",
    "\n",
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    # time: an int32 scalar raw_rnn uses to keep track of time-steps internally.\n",
    "    # previous_output: [max_time, batch_size, emb_size] tensor.\n",
    "    # previous_state: (c,h) tuple.\n",
    "    # previous_loop_state: raw_rnn uses to keep track of where it is in the loop (automatic).\n",
    "    if previous_state is None:\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "    \n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "    # *_ta: the RNN output (TensorArray <- for dynamic use)\n",
    "    # *_final_state: 2-tuple of [batch_size, emb_size] (i.e. c and h). of no use for seq2seq.\n",
    "    # _: final_loop_state, which no one gives a cupcake (used internally by *.raw_rnn backend).\n",
    "decoder_outputs = decoder_outputs_ta.stack() # [max_time, batch_size, emb_concat] \n",
    "\n",
    "decoder_max_step, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "    # for matmul, we do\n",
    "    # [max_time, batch_size, emb_concat], [max_time*batch_size, emb_concat]\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_step, decoder_batch_size, vocab_size))\n",
    "    # put it back into the original shaping scheme.\n",
    "decoder_prediction = tf.cast(tf.argmax(decoder_logits, 2), dtype=tf.int32)\n",
    "\n",
    "# Report accuracy here\n",
    "correct_raw = tf.cast(tf.equal(decoder_prediction, decoder_targets), tf.int32)\n",
    "mask = tf.cast(tf.not_equal(decoder_targets, WORD2IDX['PAD']), tf.int32) # EOSs are not 0ed out, there are BATCH_SIZE of them.\n",
    "total_seqlen = tf.cast(tf.reduce_sum(encoder_inputs_length), tf.float32)\n",
    "correct = tf.multiply(correct_raw, mask)\n",
    "accuracy = tf.cast(tf.reduce_sum(correct)-BATCH_SIZE, tf.float32) / total_seqlen # -BATCH_SIZE correction\n",
    "# accuracy = tf.cast(tf.reduce_sum(correct), tf.float32) / total_seqlen # w/o the correction.\n",
    "\n",
    "# Optimization\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits\n",
    ")\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "  minibatch loss: 2.58508086205 | accuracy -0.159999996424\n",
      "  sample 1:\n",
      "    input     > ['2', '7', '6', '9', '5', '4', '0', '1']\n",
      "    predicted > ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "    target    > ['0', '1', '2', '4', '5', '6', '7', '9', 'EOS', 'PAD', 'PAD']\n",
      "  sample 2:\n",
      "    input     > ['5', '9', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['1', '9', '9', '9', '9', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    target    > ['5', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 3:\n",
      "    input     > ['8', '6', '3', '4', '9', '2', 'PAD', 'PAD']\n",
      "    predicted > ['4', '0', '0', '0', '0', '0', '0', '0', '0', 'PAD', 'PAD']\n",
      "    target    > ['2', '3', '4', '6', '8', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "Batch 1000\n",
      "  minibatch loss: 0.247449174523 | accuracy 1.0\n",
      "  sample 1:\n",
      "    input     > ['6', '5', '8', '4', '2', '7', '1', '0', '9']\n",
      "    predicted > ['0', '1', '2', '4', '5', '6', '7', '8', '9', 'EOS', 'PAD', 'PAD']\n",
      "    target    > ['0', '1', '2', '4', '5', '6', '7', '8', '9', 'EOS', 'PAD', 'PAD']\n",
      "  sample 2:\n",
      "    input     > ['4', '6', '1', '0', '8', '2', '3', '9', '5']\n",
      "    predicted > ['0', '1', '2', '3', '4', '5', '6', '8', '9', 'EOS', 'PAD', 'PAD']\n",
      "    target    > ['0', '1', '2', '3', '4', '5', '6', '8', '9', 'EOS', 'PAD', 'PAD']\n",
      "  sample 3:\n",
      "    input     > ['3', '5', '9', '0', '8', '6', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['0', '3', '5', '6', '8', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    target    > ['0', '3', '5', '6', '8', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "Batch 2000\n",
      "  minibatch loss: 0.12578459084 | accuracy 1.0\n",
      "  sample 1:\n",
      "    input     > ['1', '7', '3', '9', '5', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['1', '3', '5', '7', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    target    > ['1', '3', '5', '7', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 2:\n",
      "    input     > ['4', '1', '7', '0', '2', '3', 'PAD', 'PAD']\n",
      "    predicted > ['0', '1', '2', '3', '4', '7', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    target    > ['0', '1', '2', '3', '4', '7', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 3:\n",
      "    input     > ['8', '9', '0', '4', '7', '3', '6', '1']\n",
      "    predicted > ['0', '1', '3', '4', '6', '7', '8', '9', 'EOS', 'PAD', 'PAD']\n",
      "    target    > ['0', '1', '3', '4', '6', '7', '8', '9', 'EOS', 'PAD', 'PAD']\n",
      "\n",
      "Batch 3000\n",
      "  minibatch loss: 0.0513751767576 | accuracy 1.0\n",
      "  sample 1:\n",
      "    input     > ['3', '0', '2', '9', '5', '8', 'PAD', 'PAD']\n",
      "    predicted > ['0', '2', '3', '5', '8', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    target    > ['0', '2', '3', '5', '8', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 2:\n",
      "    input     > ['3', '6', '9', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['3', '6', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    target    > ['3', '6', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 3:\n",
      "    input     > ['0', '8', '6', '2', '9', '4', '1', '7']\n",
      "    predicted > ['0', '1', '2', '4', '6', '7', '8', '9', 'EOS', 'PAD', 'PAD']\n",
      "    target    > ['0', '1', '2', '4', '6', '7', '8', '9', 'EOS', 'PAD', 'PAD']\n",
      "\n",
      "Batch 4000\n",
      "  minibatch loss: 0.0227461773902 | accuracy 1.0\n",
      "  sample 1:\n",
      "    input     > ['3', '7', '1', '5', '4', '9', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['1', '3', '4', '5', '7', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    target    > ['1', '3', '4', '5', '7', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 2:\n",
      "    input     > ['0', '8', '3', '1', '6', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['0', '1', '3', '6', '8', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    target    > ['0', '1', '3', '6', '8', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 3:\n",
      "    input     > ['9', '5', '6', '3', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['3', '5', '6', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    target    > ['3', '5', '6', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "Batch 5000\n",
      "  minibatch loss: 0.0156826637685 | accuracy 1.0\n",
      "  sample 1:\n",
      "    input     > ['1', '0', '7', '9', '3', '4', '8', '6', 'PAD']\n",
      "    predicted > ['0', '1', '3', '4', '6', '7', '8', '9', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "    target    > ['0', '1', '3', '4', '6', '7', '8', '9', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "  sample 2:\n",
      "    input     > ['7', '5', '6', '9', '2', '8', '0', 'PAD', 'PAD']\n",
      "    predicted > ['0', '2', '5', '6', '7', '8', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    target    > ['0', '2', '5', '6', '7', '8', '9', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 3:\n",
      "    input     > ['7', '1', '5', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['1', '5', '7', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    target    > ['1', '5', '7', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "Evaluation results (on 100 batches):\n",
      "  average loss: 0.0140012176707 | average accuracy 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def next_feed(batch_size):\n",
    "    batch_enc, batch_dec = random_batch(batch_size)\n",
    "    encoder_inputs_, encoder_inputs_lengths_ = utils.batch(batch_enc)\n",
    "    decoder_targets_, _ = utils.batch([seq + [WORD2IDX['EOS']] + [WORD2IDX['PAD']]*2 for seq in batch_dec])\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_inputs_lengths_,\n",
    "        decoder_targets: decoder_targets_\n",
    "    }\n",
    "\n",
    "loss_track = []\n",
    "\n",
    "max_batches = 5001\n",
    "batches_in_epoch = 1000\n",
    "num_test_batches = 100\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed(BATCH_SIZE)\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('Batch {}'.format(batch))\n",
    "            print('  minibatch loss: {} | accuracy {}'.format(*sess.run([loss, accuracy], fd)))\n",
    "            # predict_ = sess.run(decoder_prediction, fd)\n",
    "            predict_, lengths_ = sess.run([decoder_prediction, encoder_inputs_length], fd) # make use of seqlen\n",
    "            \n",
    "            for i, (inp, pred, tar, length) in enumerate(zip(fd[encoder_inputs].T, predict_.T, fd[decoder_targets].T, lengths_)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(decode_seq(inp)))\n",
    "                print('    predicted > {}'.format(decode_seq(pred)))\n",
    "                print('    target    > {}'.format(decode_seq(tar)))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print\n",
    "            \n",
    "    # EVALUATE ON A BIG TEST SET\n",
    "    loss_track, accuracy_track = [], []\n",
    "    for _ in range(num_test_batches):\n",
    "        fd = next_feed(BATCH_SIZE)\n",
    "        l, a = sess.run([loss, accuracy], fd)\n",
    "        loss_track.append(l)\n",
    "        accuracy_track.append(a)\n",
    "    print('Evaluation results (on {} batches):'.format(num_test_batches))\n",
    "    print('  average loss: {} | average accuracy {}'.format(np.mean(loss_track), np.mean(accuracy_track)))\n",
    "    print\n",
    "    \n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
